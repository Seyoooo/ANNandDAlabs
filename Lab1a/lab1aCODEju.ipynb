{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Classification with a single-layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Generation of linearly-separable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "\n",
    "mA = [10, 10]\n",
    "sigma = 1\n",
    "\n",
    "aX = np.random.normal(mA[0], sigma, n)\n",
    "aY = np.random.normal(mA[1], sigma, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.stack((aX, aY, np.ones(n), np.zeros(n)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mB = [5, 5]\n",
    "sigma = 1\n",
    "\n",
    "bX = np.random.normal(mB[0], sigma, n)\n",
    "bY = np.random.normal(mB[1], sigma, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.stack((bX, bY, np.ones(n), np.ones(n)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.concatenate((a, b), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(a[:, 0], a[:, 1], c='b', label='a', marker='o')\n",
    "plt.scatter(b[:, 0], b[:, 1], c='r', label='b', marker='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Classification with a single-layer perceptron and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for Perceptron learning and delta learning rule\n",
    "# learning rate and epochs are global variables\n",
    "\n",
    "# Variable dim is used to remove bias...\n",
    "def perceptron_learning(dataset, weights, batch_size, epochs, learning_rate, dim):\n",
    "    nb_goods = [0] * epochs\n",
    "    for i in range(epochs):\n",
    "        idx = 0\n",
    "        dataset_size = len(dataset)\n",
    "        while idx < dataset_size:\n",
    "            next_idx = idx + batch_size\n",
    "            if next_idx <= dataset_size:\n",
    "                y = np.matmul(weights, dataset[idx:next_idx,:dim].transpose()) > 0\n",
    "                weights = weights - learning_rate * np.matmul((y - dataset[idx:next_idx,dim]), dataset[idx:next_idx,:dim])\n",
    "            # last batch\n",
    "            else:\n",
    "                y = np.matmul(weights, dataset[idx:,:dim].transpose()) > 0\n",
    "                weights = weights - learning_rate * np.matmul((y - dataset[idx:,dim]), dataset[idx:,:dim])\n",
    "            idx = next_idx\n",
    "        nb_goods[i] = sum((np.matmul(weights, dataset[:,:dim].transpose()) > 0) == dataset[:,dim]) / dataset_size\n",
    "    return weights, nb_goods\n",
    "\n",
    "# Variable dim is used to remove bias...\n",
    "def delta_learning(dataset, weights, batch_size, epochs, learning_rate, dim):\n",
    "    nb_goods = [0] * epochs\n",
    "    for i in range(epochs):\n",
    "        idx = 0\n",
    "        dataset_size = len(dataset)\n",
    "        while idx < dataset_size:\n",
    "            next_idx = idx + batch_size\n",
    "            if next_idx <= dataset_size:\n",
    "                y = np.matmul(weights, dataset[idx:next_idx,:dim].transpose())\n",
    "                weights = weights - learning_rate * np.matmul((y - dataset[idx:next_idx,dim]), dataset[idx:next_idx,:dim])\n",
    "            # last batch\n",
    "            else:\n",
    "                y = np.matmul(weights, dataset[idx:,:dim].transpose())\n",
    "                weights = weights - learning_rate * np.matmul((y - dataset[idx:,dim]), dataset[idx:,:dim])\n",
    "            idx = next_idx\n",
    "        nb_goods[i] = sum((2*((np.matmul(weights, dataset[:,:dim].transpose()) > 0) - 0.5)) == dataset[:,dim]) / dataset_size\n",
    "    return weights, nb_goods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1 : Apply and compare perceptron learning with the delta learning rule in\n",
    "# online (sequential) mode on the generated dataset. Adjust the learning\n",
    "# rate and study the convergence of the two algorithms.\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 1000\n",
    "batch_size = 1\n",
    "\n",
    "for _ in range(3):\n",
    "    weights_pl = np.random.rand(3)\n",
    "    weights_dlr = np.copy(weights_pl)\n",
    "    xmin = min(dataset[:,0])\n",
    "    xmax = max(dataset[:,0])\n",
    "    x = np.linspace(xmin, xmax, 20)\n",
    "    x_curve = list(range(epochs))\n",
    "    \n",
    "    ### Plot of dataset\n",
    "    plt.scatter(a[:, 0], a[:, 1], c='b', marker='o', label='_nolegend_')\n",
    "    plt.scatter(b[:, 0], b[:, 1], c='r', marker='x', label='_nolegend_')\n",
    "\n",
    "    ### Perceptron learning\n",
    "    weights_pl, y_curve_perceptron = perceptron_learning(dataset, weights_pl, batch_size, epochs, learning_rate, dim=3)\n",
    "    y = (-weights_pl[2] - weights_pl[0] * x) / weights_pl[1]\n",
    "\n",
    "    plt.plot()\n",
    "    plt.plot(x, y, c='grey', linestyle='--', label='perceptron')\n",
    "\n",
    "    ### Delta learning rule\n",
    "\n",
    "    # ground truth in {-1, 1}\n",
    "    dataset_symetric = np.copy(dataset)\n",
    "    dataset_symetric[:,3] = 2 * (dataset_symetric[:,3] - 0.5)\n",
    "\n",
    "    weights_dlr, y_curve_delta = delta_learning(dataset_symetric, weights_dlr, batch_size, epochs, learning_rate, dim=3)\n",
    "    y = (-weights_dlr[2] - weights_dlr[0] * x) / weights_dlr[1]\n",
    "\n",
    "    plt.plot(x, y, c='black', linestyle='--', label='delt')\n",
    "\n",
    "    plt.legend(['Perceptron learning rule', 'Delta learning rule'])\n",
    "    plt.show()\n",
    "\n",
    "    ### Plot learning curves\n",
    "    plt.plot(x_curve, y_curve_perceptron, c='grey', label='perceptron')\n",
    "    plt.plot(x_curve, y_curve_delta, c='black', label='delta')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(['perceptron learning rule', 'delta learning rule'])\n",
    "    plt.show()\n",
    "    \n",
    "    if y_curve_perceptron[-1] < 1:\n",
    "        print(f'Perceptron proportion of good classification  : {y_curve_perceptron[-1]}')\n",
    "    if y_curve_delta[-1] < 1:\n",
    "        print(f'Delta proportion of good classification :  {y_curve_delta[-1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different batch size for deltat rule\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "\n",
    "for _ in range(3):\n",
    "    weights_pl = np.random.rand(3)\n",
    "    weights_dlr = np.copy(weights_pl)\n",
    "    xmin = min(dataset[:,0])\n",
    "    xmax = max(dataset[:,0])\n",
    "    x = np.linspace(xmin, xmax, 20)\n",
    "    x_curve = list(range(epochs))\n",
    "\n",
    "    ### Delta learning rule\n",
    "\n",
    "    # ground truth in {-1, 1}\n",
    "    dataset_symetric = np.copy(dataset)\n",
    "    dataset_symetric[:,3] = 2 * (dataset_symetric[:,3] - 0.5)\n",
    "\n",
    "    legend = []\n",
    "    for batch_size, color in [(1, 'black'), (3, 'grey'), (5, 'green'), (10, 'blue')]:\n",
    "        _, y_curve_delta = delta_learning(dataset_symetric, weights_dlr, batch_size, epochs, learning_rate, dim=3)\n",
    "        plt.plot(x_curve, y_curve_delta, c=color)\n",
    "        legend.append(f'batch size : {batch_size}')\n",
    "\n",
    "    # plt.plot(x_curve, [1]*len(x_curve), color='pink')\n",
    "\n",
    "    plt.legend(legend)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the bias\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 1\n",
    "\n",
    "dataset_no_bias = np.delete(np.copy(dataset), 2, 1)\n",
    "dataset_symetric_no_bias = np.delete(np.copy(dataset_symetric), 2, 1)\n",
    "\n",
    "for _ in range(3):\n",
    "    weights_pl = np.random.rand(2)\n",
    "    weights_dlr = np.copy(weights_pl)\n",
    "    xmin = min(dataset[:,0])\n",
    "    xmax = max(dataset[:,0])\n",
    "    x = np.linspace(xmin, xmax, 20)\n",
    "    x_curve = list(range(epochs))\n",
    "    \n",
    "    ### Plot of dataset\n",
    "    plt.scatter(a[:, 0], a[:, 1], c='b', label='a', marker='o')\n",
    "    plt.scatter(b[:, 0], b[:, 1], c='r', label='b', marker='x')\n",
    "\n",
    "    ### Perceptron learning\n",
    "    weights_pl, y_curve_perceptron = perceptron_learning(dataset_no_bias, weights_pl, batch_size, epochs, learning_rate, dim=2)\n",
    "    y = ( - weights_pl[0] * x) / weights_pl[1]\n",
    "\n",
    "    plt.plot()\n",
    "    plt.plot(x, y, c='grey', linestyle='--')\n",
    "\n",
    "    ### Delta learning rule\n",
    "\n",
    "    weights_dlr, y_curve_delta = delta_learning(dataset_symetric_no_bias, weights_dlr, batch_size, epochs, learning_rate, dim=2)\n",
    "    y = ( - weights_dlr[0] * x) / weights_dlr[1]\n",
    "\n",
    "    plt.plot(x, y, c='black', linestyle='--')\n",
    "    plt.show()\n",
    "\n",
    "    ### Plot learning curves\n",
    "    plt.plot(x_curve, y_curve_perceptron, c='grey')\n",
    "    plt.plot(x_curve, y_curve_delta, c='black')\n",
    "    plt.show()\n",
    "    \n",
    "    if y_curve_perceptron[-1] < 1:\n",
    "        print(f'Perceptron proportion of good classification  : {y_curve_perceptron[-1]}')\n",
    "    if y_curve_delta[-1] < 1:\n",
    "        print(f'Delta proportion of good classification :  {y_curve_delta[-1]}')\n",
    "    \n",
    "    print(weights_dlr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed010600908602893e54c2254557a102f13966df447e3060692476b3d496eb5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

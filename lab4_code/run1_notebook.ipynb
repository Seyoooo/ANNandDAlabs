{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "\n",
    "class RestrictedBoltzmannMachine():\n",
    "    '''\n",
    "    For more details : A Practical Guide to Training Restricted Boltzmann Machines https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf\n",
    "    '''\n",
    "    def __init__(self, ndim_visible, ndim_hidden, is_bottom=False, image_size=[28,28], is_top=False, n_labels=10, batch_size=10):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          ndim_visible: Number of units in visible layer.\n",
    "          ndim_hidden: Number of units in hidden layer.\n",
    "          is_bottom: True only if this rbm is at the bottom of the stack in a deep belief net. Used to interpret visible layer as image data with dimensions \"image_size\".\n",
    "          image_size: Image dimension for visible layer.\n",
    "          is_top: True only if this rbm is at the top of stack in deep beleif net. Used to interpret visible layer as concatenated with \"n_label\" unit of label data at the end. \n",
    "          n_label: Number of label categories.\n",
    "          batch_size: Size of mini-batch.\n",
    "        \"\"\"\n",
    "       \n",
    "        self.ndim_visible = ndim_visible\n",
    "\n",
    "        self.ndim_hidden = ndim_hidden\n",
    "\n",
    "        self.is_bottom = is_bottom\n",
    "        if is_bottom : self.image_size = image_size\n",
    "        \n",
    "        self.is_top = is_top\n",
    "\n",
    "        if is_top : self.n_labels = 10\n",
    "\n",
    "        self.batch_size = batch_size        \n",
    "                \n",
    "        self.delta_bias_v = 0\n",
    "\n",
    "        self.delta_weight_vh = 0\n",
    "\n",
    "        self.delta_bias_h = 0\n",
    "\n",
    "        self.bias_v = np.random.normal(loc=0.0, scale=0.01, size=(self.ndim_visible))\n",
    "\n",
    "        self.weight_vh = np.random.normal(loc=0.0, scale=0.01, size=(self.ndim_visible,self.ndim_hidden))\n",
    "\n",
    "        self.bias_h = np.random.normal(loc=0.0, scale=0.01, size=(self.ndim_hidden))\n",
    "        \n",
    "        self.delta_weight_v_to_h = 0\n",
    "\n",
    "        self.delta_weight_h_to_v = 0        \n",
    "        \n",
    "        self.weight_v_to_h = None\n",
    "        \n",
    "        self.weight_h_to_v = None\n",
    "\n",
    "        self.learning_rate = 0.01\n",
    "        \n",
    "        self.momentum = 0.7\n",
    "\n",
    "        self.print_period = 5000\n",
    "        \n",
    "        self.rf = { # receptive-fields. Only applicable when visible layer is input data\n",
    "            \"period\" : 5000, # iteration period to visualize\n",
    "            \"grid\" : [5,5], # size of the grid\n",
    "            \"ids\" : np.random.randint(0,self.ndim_hidden,25) # pick some random hidden units\n",
    "            }\n",
    "        \n",
    "        return\n",
    "\n",
    "        \n",
    "    def cd1(self, visible_trainset, n_iterations=10000):\n",
    "        \n",
    "        \"\"\"Contrastive Divergence with k=1 full alternating Gibbs sampling\n",
    "\n",
    "        Args:\n",
    "          visible_trainset: training data for this rbm, shape is (size of training set, size of visible layer)\n",
    "          n_iterations: number of iterations of learning (each iteration learns a mini-batch)\n",
    "        \"\"\"\n",
    "\n",
    "        print (\"learning CD1\")\n",
    "        \n",
    "        n_samples = visible_trainset.shape[0]\n",
    "        index = 0 \n",
    "\n",
    "        for it in range(1):\n",
    "            # Select next mini-batch\n",
    "            next_index = index + self.batch_size\n",
    "\n",
    "            if next_index < n_samples:\n",
    "                v_0 = visible_trainset[index:next_index]\n",
    "            else:\n",
    "                v_0 = np.concatenate((visible_trainset[index:],visible_trainset[:next_index-n_samples]))\n",
    "                print(index)\n",
    "            index = next_index % n_samples\n",
    "\n",
    "\t        # [Done TASK 4.1] run k=1 alternating Gibbs sampling : v_0 -> h_0 ->  v_1 -> h_1.\n",
    "            # you may need to use the inference functions 'get_h_given_v' and 'get_v_given_h'.\n",
    "            # note that inference methods returns both probabilities and activations (samples from probablities) and you may have to decide when to use what.\n",
    "\n",
    "            print(v_0)\n",
    "\n",
    "            h_0_prob, h_0_bin = self.get_h_given_v(v_0)\n",
    "            v_1_prob, v_1_bin = self.get_v_given_h(h_0_prob)\n",
    "            h_1_prob, h_1_bin = self.get_h_given_v(v_1_bin)\n",
    "            \n",
    "            # [Done TASK 4.1] update the parameters using function 'update_params'\n",
    "            self.update_params(v_0, h_0_bin, v_1_prob, h_1_prob)\n",
    "\n",
    "            # visualize once in a while when visible layer is input images\n",
    "            \n",
    "            if it % self.rf[\"period\"] == 0 and self.is_bottom:\n",
    "                viz_rf(weights=self.weight_vh[:,self.rf[\"ids\"]].reshape((self.image_size[0],self.image_size[1],-1)), it=it, grid=self.rf[\"grid\"])\n",
    "\n",
    "            # print progress\n",
    "            \n",
    "            if it % self.print_period == 0 :\n",
    "                print (\"iteration=%7d recon_loss=%4.4f\"%(it, np.linalg.norm(visible_trainset - visible_trainset)))\n",
    "        \n",
    "        return\n",
    "    \n",
    "\n",
    "    def update_params(self,v_0,h_0,v_k,h_k):\n",
    "\n",
    "        \"\"\"Update the weight and bias parameters.\n",
    "\n",
    "        You could also add weight decay and momentum for weight updates.\n",
    "\n",
    "        Args:\n",
    "           v_0: activities or probabilities of visible layer (data to the rbm)\n",
    "           h_0: activities or probabilities of hidden layer\n",
    "           v_k: activities or probabilities of visible layer\n",
    "           h_k: activities or probabilities of hidden layer\n",
    "           all args have shape (size of mini-batch, size of respective layer)\n",
    "        \"\"\"\n",
    "\n",
    "        # [DONE TASK 4.1] get the gradients from the arguments (replace the 0s below) and update the weight and bias parameters\n",
    "        \n",
    "        self.delta_bias_v = self.learning_rate * np.mean(v_0 - v_k, axis=0)\n",
    "        self.delta_weight_vh = self.learning_rate * (v_0.T @ h_0 - v_k.T @ h_k) / v_0.shape[0]\n",
    "        self.delta_bias_h = self.learning_rate * np.mean(h_0 - h_k, axis=0)\n",
    "\n",
    "        assert self.delta_bias_v.shape[0] == v_0.shape[1]\n",
    "\n",
    "        self.bias_v += self.delta_bias_v\n",
    "        self.weight_vh += self.delta_weight_vh\n",
    "        self.bias_h += self.delta_bias_h\n",
    "        \n",
    "        return\n",
    "\n",
    "    def get_h_given_v(self, visible_minibatch):\n",
    "        \n",
    "        \"\"\"Compute probabilities p(h|v) and activations h ~ p(h|v) \n",
    "\n",
    "        Uses undirected weight \"weight_vh\" and bias \"bias_h\"\n",
    "        \n",
    "        Args: \n",
    "           visible_minibatch: shape is (size of mini-batch, size of visible layer)\n",
    "        Returns:        \n",
    "           tuple ( p(h|v) , h) \n",
    "           both are shaped (size of mini-batch, size of hidden layer)\n",
    "        \"\"\"\n",
    "        \n",
    "        assert self.weight_vh is not None\n",
    "\n",
    "        n_samples = visible_minibatch.shape[0]\n",
    "\n",
    "        # [Done TASK 4.1] compute probabilities and activations (samples from probabilities) of hidden layer (replace the zeros below) \n",
    "\n",
    "        probs = sigmoid(visible_minibatch @ self.weight_vh + self.bias_h)\n",
    "        binary_states = np.random.binomial(1, probs, size=None)\n",
    "        \n",
    "        return probs, binary_states\n",
    "\n",
    "\n",
    "    def get_v_given_h(self,hidden_minibatch):\n",
    "        \n",
    "        \"\"\"Compute probabilities p(v|h) and activations v ~ p(v|h)\n",
    "\n",
    "        Uses undirected weight \"weight_vh\" and bias \"bias_v\"\n",
    "        \n",
    "        Args: \n",
    "           hidden_minibatch: shape is (size of mini-batch, size of hidden layer)\n",
    "        Returns:        \n",
    "           tuple ( p(v|h) , v) \n",
    "           both are shaped (size of mini-batch, size of visible layer)\n",
    "        \"\"\"\n",
    "        \n",
    "        assert self.weight_vh is not None\n",
    "\n",
    "        n_samples = hidden_minibatch.shape[0]\n",
    "\n",
    "        if self.is_top:\n",
    "\n",
    "            \"\"\"\n",
    "            Here visible layer has both data and labels. Compute total input for each unit (identical for both cases), \\ \n",
    "            and split into two parts, something like support[:, :-self.n_labels] and support[:, -self.n_labels:]. \\\n",
    "            Then, for both parts, use the appropriate activation function to get probabilities and a sampling method \\\n",
    "            to get activities. The probabilities as well as activities can then be concatenated back into a normal visible layer.\n",
    "            \"\"\"\n",
    "\n",
    "            # [TODO TASK 4.1] compute probabilities and activations (samples from probabilities) of visible layer (replace the pass below). \\\n",
    "            # Note that this section can also be postponed until TASK 4.2, since in this task, stand-alone RBMs do not contain labels in visible layer.\n",
    "            \n",
    "            \n",
    "            pass\n",
    "            \n",
    "        else:\n",
    "                        \n",
    "            # [DONE TASK 4.1] compute probabilities and activations (samples from probabilities) of visible layer (replace the pass and zeros below)\n",
    "\n",
    "            probs = sigmoid(hidden_minibatch @ self.weight_vh.T + self.bias_v)\n",
    "            binary_states = np.random.binomial(1, probs, size=None)             \n",
    "\n",
    "            pass\n",
    "        \n",
    "        return probs, binary_states\n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\" rbm as a belief layer : the functions below do not have to be changed until running a deep belief net \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    def untwine_weights(self):\n",
    "        \n",
    "        self.weight_v_to_h = np.copy( self.weight_vh )\n",
    "        self.weight_h_to_v = np.copy( np.transpose(self.weight_vh) )\n",
    "        self.weight_vh = None\n",
    "\n",
    "    def get_h_given_v_dir(self,visible_minibatch):\n",
    "\n",
    "        \"\"\"Compute probabilities p(h|v) and activations h ~ p(h|v)\n",
    "\n",
    "        Uses directed weight \"weight_v_to_h\" and bias \"bias_h\"\n",
    "        \n",
    "        Args: \n",
    "           visible_minibatch: shape is (size of mini-batch, size of visible layer)\n",
    "        Returns:        \n",
    "           tuple ( p(h|v) , h) \n",
    "           both are shaped (size of mini-batch, size of hidden layer)\n",
    "        \"\"\"\n",
    "        \n",
    "        assert self.weight_v_to_h is not None\n",
    "\n",
    "        n_samples = visible_minibatch.shape[0]\n",
    "\n",
    "        # [TODO TASK 4.2] perform same computation as the function 'get_h_given_v' but with directed connections (replace the zeros below) \n",
    "        \n",
    "        return np.zeros((n_samples,self.ndim_hidden)), np.zeros((n_samples,self.ndim_hidden))\n",
    "\n",
    "\n",
    "    def get_v_given_h_dir(self,hidden_minibatch):\n",
    "\n",
    "\n",
    "        \"\"\"Compute probabilities p(v|h) and activations v ~ p(v|h)\n",
    "\n",
    "        Uses directed weight \"weight_h_to_v\" and bias \"bias_v\"\n",
    "        \n",
    "        Args: \n",
    "           hidden_minibatch: shape is (size of mini-batch, size of hidden layer)\n",
    "        Returns:        \n",
    "           tuple ( p(v|h) , v) \n",
    "           both are shaped (size of mini-batch, size of visible layer)\n",
    "        \"\"\"\n",
    "        \n",
    "        assert self.weight_h_to_v is not None\n",
    "        \n",
    "        n_samples = hidden_minibatch.shape[0]\n",
    "        \n",
    "        if self.is_top:\n",
    "\n",
    "            \"\"\"\n",
    "            Here visible layer has both data and labels. Compute total input for each unit (identical for both cases), \\ \n",
    "            and split into two parts, something like support[:, :-self.n_labels] and support[:, -self.n_labels:]. \\\n",
    "            Then, for both parts, use the appropriate activation function to get probabilities and a sampling method \\\n",
    "            to get activities. The probabilities as well as activities can then be concatenated back into a normal visible layer.\n",
    "            \"\"\"\n",
    "            \n",
    "            # [TODO TASK 4.2] Note that even though this function performs same computation as 'get_v_given_h' but with directed connections,\n",
    "            # this case should never be executed : when the RBM is a part of a DBN and is at the top, it will have not have directed connections.\n",
    "            # Appropriate code here is to raise an error (replace pass below)\n",
    "            \n",
    "            pass\n",
    "            \n",
    "        else:\n",
    "                        \n",
    "            # [TODO TASK 4.2] performs same computaton as the function 'get_v_given_h' but with directed connections (replace the pass and zeros below)             \n",
    "\n",
    "            pass\n",
    "            \n",
    "        return np.zeros((n_samples,self.ndim_visible)), np.zeros((n_samples,self.ndim_visible))        \n",
    "        \n",
    "    def update_generate_params(self,inps,trgs,preds):\n",
    "        \n",
    "        \"\"\"Update generative weight \"weight_h_to_v\" and bias \"bias_v\"\n",
    "        \n",
    "        Args:\n",
    "           inps: activities or probabilities of input unit\n",
    "           trgs: activities or probabilities of output unit (target)\n",
    "           preds: activities or probabilities of output unit (prediction)\n",
    "           all args have shape (size of mini-batch, size of respective layer)\n",
    "        \"\"\"\n",
    "\n",
    "        # [TODO TASK 4.3] find the gradients from the arguments (replace the 0s below) and update the weight and bias parameters.\n",
    "        \n",
    "        self.delta_weight_h_to_v += 0\n",
    "        self.delta_bias_v += 0\n",
    "        \n",
    "        self.weight_h_to_v += self.delta_weight_h_to_v\n",
    "        self.bias_v += self.delta_bias_v \n",
    "        \n",
    "        return\n",
    "    \n",
    "    def update_recognize_params(self,inps,trgs,preds):\n",
    "        \n",
    "        \"\"\"Update recognition weight \"weight_v_to_h\" and bias \"bias_h\"\n",
    "        \n",
    "        Args:\n",
    "           inps: activities or probabilities of input unit\n",
    "           trgs: activities or probabilities of output unit (target)\n",
    "           preds: activities or probabilities of output unit (prediction)\n",
    "           all args have shape (size of mini-batch, size of respective layer)\n",
    "        \"\"\"\n",
    "\n",
    "        # [TODO TASK 4.3] find the gradients from the arguments (replace the 0s below) and update the weight and bias parameters.\n",
    "\n",
    "        self.delta_weight_v_to_h += 0\n",
    "        self.delta_bias_h += 0\n",
    "\n",
    "        self.weight_v_to_h += self.delta_weight_v_to_h\n",
    "        self.bias_h += self.delta_bias_h\n",
    "        \n",
    "        return    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting a Restricted Boltzmann Machine..\n"
     ]
    }
   ],
   "source": [
    "image_size = [28,28]\n",
    "train_imgs,train_lbls,test_imgs,test_lbls = read_mnist(dim=image_size, n_train=60000, n_test=10000)\n",
    "\n",
    "''' restricted boltzmann machine '''\n",
    "\n",
    "print (\"\\nStarting a Restricted Boltzmann Machine..\")\n",
    "\n",
    "rbm = RestrictedBoltzmannMachine(ndim_visible=image_size[0]*image_size[1],\n",
    "                                    ndim_hidden=200,\n",
    "                                    is_bottom=True,\n",
    "                                    image_size=image_size,\n",
    "                                    is_top=False,\n",
    "                                    n_labels=10,\n",
    "                                    batch_size=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "visible_trainset = train_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = visible_trainset.shape[0]\n",
    "index = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_index = index + rbm.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if next_index < n_samples:\n",
    "    v_0 = visible_trainset[index:next_index]\n",
    "else:\n",
    "    v_0 = np.concatenate((visible_trainset[index:],visible_trainset[:next_index-n_samples]))\n",
    "    print(index)\n",
    "index = next_index % n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "         0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "         0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "         0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "         1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "         1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "         0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "         1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "         1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 1]]),\n",
       " array([[0.46457698, 0.49697262, 0.49671357, 0.51406587, 0.52268889,\n",
       "         0.47675835, 0.52013394, 0.48928487, 0.45233671, 0.50509469,\n",
       "         0.52744822, 0.53031384, 0.47647737, 0.48563413, 0.5252661 ,\n",
       "         0.49514727, 0.5055053 , 0.48871174, 0.49695796, 0.51598274,\n",
       "         0.4815522 , 0.49712749, 0.50336099, 0.53222483, 0.51937184,\n",
       "         0.53055567, 0.49651146, 0.49669511, 0.50286694, 0.51899675,\n",
       "         0.47509036, 0.51828439, 0.46321495, 0.49552894, 0.53561196,\n",
       "         0.52445321, 0.52540779, 0.48256845, 0.463992  , 0.47981915,\n",
       "         0.49343682, 0.4702791 , 0.49519303, 0.51763037, 0.50077606,\n",
       "         0.50338453, 0.49096507, 0.49375998, 0.4993593 , 0.52012711,\n",
       "         0.48230424, 0.51867815, 0.47451244, 0.52876851, 0.51886214,\n",
       "         0.52744135, 0.5145205 , 0.51268397, 0.53495984, 0.48957685,\n",
       "         0.46906676, 0.51504953, 0.49060318, 0.4878931 , 0.51976252,\n",
       "         0.5106678 , 0.46686036, 0.50190094, 0.53424928, 0.50435385,\n",
       "         0.50882824, 0.4924022 , 0.49951055, 0.52688902, 0.54376215,\n",
       "         0.50261538, 0.43882406, 0.50713853, 0.5140949 , 0.47979195,\n",
       "         0.49307318, 0.49488145, 0.5158182 , 0.50787428, 0.51865274,\n",
       "         0.52996294, 0.47394109, 0.46845877, 0.45963979, 0.51282263,\n",
       "         0.54160882, 0.49613329, 0.52379826, 0.54287497, 0.53339334,\n",
       "         0.48188614, 0.51028114, 0.51136377, 0.50773213, 0.50597063,\n",
       "         0.49035986, 0.50911142, 0.53563667, 0.5118825 , 0.47492362,\n",
       "         0.44869032, 0.43833475, 0.46601461, 0.47245043, 0.48324811,\n",
       "         0.50500697, 0.51738381, 0.48265718, 0.53836826, 0.46506026,\n",
       "         0.49896774, 0.50343831, 0.50162754, 0.47423104, 0.43919473,\n",
       "         0.48376522, 0.47961407, 0.51443937, 0.52151644, 0.52342913,\n",
       "         0.53532594, 0.50495814, 0.49057654, 0.49793772, 0.47890271,\n",
       "         0.50380078, 0.47636329, 0.52314566, 0.50641308, 0.5468875 ,\n",
       "         0.4953413 , 0.53418612, 0.51144708, 0.5424437 , 0.51939513,\n",
       "         0.4729221 , 0.5057148 , 0.49324818, 0.46703889, 0.52975718,\n",
       "         0.49870143, 0.50730502, 0.49427322, 0.51094179, 0.5304099 ,\n",
       "         0.47305858, 0.50756281, 0.44913287, 0.5181092 , 0.47875579,\n",
       "         0.52947926, 0.47282902, 0.48851014, 0.55017082, 0.47498813,\n",
       "         0.49537629, 0.52176658, 0.47752398, 0.44209024, 0.52908034,\n",
       "         0.47955222, 0.52134858, 0.46206823, 0.53201758, 0.50458898,\n",
       "         0.54045376, 0.50413377, 0.52843549, 0.48070696, 0.50477628,\n",
       "         0.4820925 , 0.54748301, 0.49644994, 0.50117955, 0.51698318,\n",
       "         0.50951296, 0.52486856, 0.52869605, 0.50495157, 0.50177811,\n",
       "         0.48951479, 0.50866692, 0.48556069, 0.47143131, 0.51340841,\n",
       "         0.47490972, 0.47595946, 0.49506433, 0.47054971, 0.46560474,\n",
       "         0.49881691, 0.51627219, 0.5093267 , 0.48118665, 0.52594605]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_0_prob, h_0_bin = rbm.get_h_given_v(v_0)\n",
    "h_0_bin, h_0_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.51859372, 0.52708026, 0.51145361, 0.50539682, 0.51989391,\n",
       "         0.47328972, 0.51115767, 0.49147438, 0.52741735, 0.52600718,\n",
       "         0.49576255, 0.50500476, 0.49554818, 0.4831602 , 0.50514371,\n",
       "         0.51847034, 0.46825674, 0.51251083, 0.54221939, 0.51748312,\n",
       "         0.46517445, 0.48731263, 0.53785792, 0.50879864, 0.52196705,\n",
       "         0.48711381, 0.51875704, 0.53681066, 0.5159137 , 0.52492783,\n",
       "         0.49502331, 0.50015567, 0.5303455 , 0.51110817, 0.54799885,\n",
       "         0.49900224, 0.53290973, 0.47790792, 0.47970638, 0.49339148,\n",
       "         0.50823776, 0.507716  , 0.4767085 , 0.48385257, 0.51055709,\n",
       "         0.49888516, 0.48491613, 0.47622552, 0.52236081, 0.49820741,\n",
       "         0.50742376, 0.50183798, 0.51336134, 0.51468213, 0.47596284,\n",
       "         0.49941286, 0.4903138 , 0.51230829, 0.51430776, 0.49266989,\n",
       "         0.48967819, 0.4856134 , 0.4980682 , 0.47612477, 0.49015931,\n",
       "         0.50320566, 0.47516042, 0.50501029, 0.48829282, 0.48574115,\n",
       "         0.48517929, 0.49674043, 0.47136277, 0.52500547, 0.4967683 ,\n",
       "         0.47240705, 0.50993868, 0.50624165, 0.49899585, 0.51771203,\n",
       "         0.48059683, 0.48941478, 0.49766775, 0.48881539, 0.51415546,\n",
       "         0.46822039, 0.51055678, 0.48667434, 0.49681328, 0.50541623,\n",
       "         0.51443776, 0.5035122 , 0.5030542 , 0.50965191, 0.54434951,\n",
       "         0.46837829, 0.50083701, 0.51199848, 0.50946672, 0.48448387,\n",
       "         0.47889128, 0.52421354, 0.5134178 , 0.48856825, 0.49992868,\n",
       "         0.4867385 , 0.49487624, 0.48119195, 0.52401188, 0.49858689,\n",
       "         0.50133674, 0.51960323, 0.48256831, 0.49658667, 0.49522819,\n",
       "         0.5028535 , 0.48094591, 0.52901054, 0.49372871, 0.49692847,\n",
       "         0.49649087, 0.49021828, 0.51859789, 0.48413673, 0.52905267,\n",
       "         0.52200543, 0.49099285, 0.4795513 , 0.54531028, 0.50119613,\n",
       "         0.49166969, 0.4934804 , 0.50590238, 0.4950003 , 0.46911979,\n",
       "         0.49598051, 0.48732404, 0.50725448, 0.4705976 , 0.50072981,\n",
       "         0.48586332, 0.52656548, 0.47011639, 0.50398904, 0.51469443,\n",
       "         0.5127052 , 0.49233012, 0.49426845, 0.49405634, 0.51256246,\n",
       "         0.50978047, 0.4577229 , 0.51306577, 0.51145896, 0.52336996,\n",
       "         0.46364294, 0.50916112, 0.49355863, 0.49628889, 0.50365053,\n",
       "         0.49038447, 0.47893125, 0.47942963, 0.49202323, 0.50732479,\n",
       "         0.48860801, 0.48656952, 0.49775738, 0.51668511, 0.52125901,\n",
       "         0.5245164 , 0.49879975, 0.52430487, 0.49744367, 0.49137266,\n",
       "         0.46989604, 0.47965333, 0.49095638, 0.46859502, 0.494694  ,\n",
       "         0.50199538, 0.51072349, 0.52964122, 0.50478926, 0.5005549 ,\n",
       "         0.52321631, 0.52110278, 0.49306052, 0.49011162, 0.50579542,\n",
       "         0.50267271, 0.49158491, 0.48935866, 0.50650496, 0.52429306,\n",
       "         0.47293492, 0.52362602, 0.54338564, 0.49799333, 0.54124369,\n",
       "         0.4988009 , 0.48046272, 0.48086551, 0.48774379, 0.50251729,\n",
       "         0.49056172, 0.51984262, 0.52274344, 0.5090254 , 0.51113928,\n",
       "         0.48804952, 0.48650387, 0.51569351, 0.48820049, 0.48054379,\n",
       "         0.50841522, 0.50900171, 0.49359579, 0.4799169 , 0.47333347,\n",
       "         0.48309837, 0.49374206, 0.46839636, 0.50216077, 0.49897982,\n",
       "         0.47729888, 0.48363627, 0.48943779, 0.51441452, 0.49427373,\n",
       "         0.48420719, 0.48514812, 0.50362144, 0.49509956, 0.49384274,\n",
       "         0.53664824, 0.50092597, 0.47366807, 0.52987011, 0.50474411,\n",
       "         0.49010196, 0.52334128, 0.50313744, 0.51257447, 0.5022546 ,\n",
       "         0.51192729, 0.52397408, 0.52107755, 0.51363385, 0.48305387,\n",
       "         0.51836496, 0.48467573, 0.55038307, 0.49620876, 0.49383572,\n",
       "         0.50815745, 0.49353873, 0.51650237, 0.50237775, 0.52354494,\n",
       "         0.48558831, 0.4875222 , 0.49560888, 0.52550479, 0.49106369,\n",
       "         0.49618211, 0.51412827, 0.5048521 , 0.52812026, 0.47805325,\n",
       "         0.52514799, 0.49634901, 0.48550005, 0.48573006, 0.48719626,\n",
       "         0.51197936, 0.50512365, 0.4979599 , 0.48238335, 0.49536314,\n",
       "         0.50875566, 0.49041168, 0.48836758, 0.49660443, 0.49042417,\n",
       "         0.51145015, 0.49124301, 0.49489725, 0.5291979 , 0.46212257,\n",
       "         0.48760155, 0.51785341, 0.5347097 , 0.502492  , 0.49910054,\n",
       "         0.49995299, 0.51466536, 0.52724798, 0.48353562, 0.51617845,\n",
       "         0.53129389, 0.53116848, 0.50261788, 0.50764635, 0.52588193,\n",
       "         0.50203571, 0.47773795, 0.51316625, 0.51959255, 0.50383296,\n",
       "         0.50685454, 0.49917496, 0.52039989, 0.51471408, 0.50714974,\n",
       "         0.50452744, 0.47427706, 0.50855787, 0.493689  , 0.51795291,\n",
       "         0.5239167 , 0.51712076, 0.51485482, 0.49733104, 0.44609914,\n",
       "         0.5142364 , 0.54070751, 0.52767451, 0.49303065, 0.46137968,\n",
       "         0.49945391, 0.51366586, 0.47586268, 0.48911666, 0.50317161,\n",
       "         0.52494629, 0.49659002, 0.49710992, 0.4883071 , 0.47637947,\n",
       "         0.51876533, 0.47652144, 0.48708466, 0.50057404, 0.46435154,\n",
       "         0.48895271, 0.53513473, 0.51740851, 0.4768437 , 0.49534297,\n",
       "         0.47818222, 0.50230026, 0.50001706, 0.50185573, 0.49682571,\n",
       "         0.49640813, 0.4925222 , 0.50773397, 0.51949874, 0.48351172,\n",
       "         0.49647255, 0.50776885, 0.48476335, 0.51212836, 0.51419421,\n",
       "         0.4908765 , 0.49356083, 0.48008616, 0.48404096, 0.48986344,\n",
       "         0.49052245, 0.48543769, 0.5143202 , 0.50526357, 0.50869924,\n",
       "         0.50752053, 0.48852585, 0.4758076 , 0.48846581, 0.51943054,\n",
       "         0.4841112 , 0.5172178 , 0.49467739, 0.47091952, 0.4954711 ,\n",
       "         0.4966823 , 0.50129012, 0.49947186, 0.52115875, 0.47020473,\n",
       "         0.46629835, 0.51537908, 0.52906536, 0.48597898, 0.52391961,\n",
       "         0.51459635, 0.50824746, 0.49980566, 0.52062518, 0.55018273,\n",
       "         0.47688682, 0.50080654, 0.50128074, 0.51278029, 0.50928167,\n",
       "         0.51093742, 0.50536974, 0.5093587 , 0.48389233, 0.48528634,\n",
       "         0.50663162, 0.52421587, 0.48486951, 0.4912694 , 0.46064262,\n",
       "         0.4871828 , 0.50935838, 0.47402435, 0.48364204, 0.49920671,\n",
       "         0.49588485, 0.48519656, 0.49220981, 0.48218935, 0.50357703,\n",
       "         0.49767535, 0.49308947, 0.52729367, 0.4794391 , 0.49181294,\n",
       "         0.50361604, 0.48940487, 0.51226428, 0.47920762, 0.50133849,\n",
       "         0.47419775, 0.50666554, 0.48395769, 0.51656048, 0.48159467,\n",
       "         0.51883307, 0.50745672, 0.48899153, 0.52784438, 0.47087652,\n",
       "         0.51773522, 0.50721592, 0.48050677, 0.49181877, 0.48408732,\n",
       "         0.52367598, 0.47296015, 0.52306891, 0.46565202, 0.5077647 ,\n",
       "         0.50204644, 0.51091875, 0.51086568, 0.52111508, 0.52568919,\n",
       "         0.48974792, 0.45890147, 0.48558401, 0.47615367, 0.50965079,\n",
       "         0.50424084, 0.50855082, 0.50093829, 0.47209465, 0.5155223 ,\n",
       "         0.50801151, 0.50594704, 0.51399366, 0.50849038, 0.5035989 ,\n",
       "         0.51668125, 0.51187627, 0.52284589, 0.48596361, 0.48466734,\n",
       "         0.50447025, 0.47372771, 0.48375664, 0.50462956, 0.50851464,\n",
       "         0.51704527, 0.50270828, 0.50394346, 0.49983825, 0.48716747,\n",
       "         0.52520285, 0.50318158, 0.50747002, 0.49210364, 0.532627  ,\n",
       "         0.52867705, 0.48074135, 0.48448582, 0.50740894, 0.48094572,\n",
       "         0.48744803, 0.48449989, 0.50183945, 0.47454458, 0.49261066,\n",
       "         0.51357439, 0.44617971, 0.4990723 , 0.52267043, 0.46372337,\n",
       "         0.51256471, 0.53356514, 0.52010481, 0.50443606, 0.53732345,\n",
       "         0.503269  , 0.50626336, 0.52226708, 0.49975843, 0.50006655,\n",
       "         0.50249686, 0.50599927, 0.4842238 , 0.49580679, 0.49510092,\n",
       "         0.49308508, 0.48340185, 0.48870771, 0.48750156, 0.47234572,\n",
       "         0.50975074, 0.5137012 , 0.5036005 , 0.47063773, 0.51393827,\n",
       "         0.51951882, 0.46069593, 0.5155016 , 0.49112456, 0.50088151,\n",
       "         0.51365089, 0.49060874, 0.46559842, 0.48532555, 0.46712402,\n",
       "         0.48107578, 0.49711113, 0.48550094, 0.49505441, 0.46694341,\n",
       "         0.48602335, 0.49607155, 0.48397452, 0.53672328, 0.5125055 ,\n",
       "         0.50846495, 0.50541022, 0.51915196, 0.47473424, 0.49818993,\n",
       "         0.48161466, 0.54607313, 0.526587  , 0.49174648, 0.49429705,\n",
       "         0.52234588, 0.46054703, 0.54685605, 0.50175941, 0.47126945,\n",
       "         0.47426559, 0.49276761, 0.48018529, 0.46003506, 0.51046553,\n",
       "         0.51674052, 0.52305616, 0.52384289, 0.50882905, 0.50689578,\n",
       "         0.50449312, 0.50546502, 0.47620194, 0.55471618, 0.49396518,\n",
       "         0.48126345, 0.50181798, 0.51916912, 0.47335866, 0.50464251,\n",
       "         0.48130515, 0.54202473, 0.49727457, 0.47091019, 0.49968482,\n",
       "         0.48975011, 0.50001911, 0.52506974, 0.53716684, 0.50218497,\n",
       "         0.51897739, 0.48920662, 0.48133017, 0.52300295, 0.52008186,\n",
       "         0.52668537, 0.52601406, 0.5034676 , 0.45881547, 0.48410372,\n",
       "         0.50251851, 0.48614429, 0.48174413, 0.52804727, 0.49807372,\n",
       "         0.49263479, 0.48214303, 0.52127695, 0.49323115, 0.49646503,\n",
       "         0.53162186, 0.49527412, 0.51302225, 0.49710089, 0.5231352 ,\n",
       "         0.50037374, 0.51311335, 0.50649781, 0.48388947, 0.50012086,\n",
       "         0.50097297, 0.52559871, 0.50681285, 0.49938007, 0.49433986,\n",
       "         0.4997728 , 0.48559028, 0.48261249, 0.49725307, 0.51724829,\n",
       "         0.49055966, 0.50104751, 0.49677351, 0.49583049, 0.51142967,\n",
       "         0.53540364, 0.49716886, 0.49638213, 0.51515501, 0.49273716,\n",
       "         0.49472433, 0.54227095, 0.53350676, 0.49387032, 0.51516275,\n",
       "         0.48523023, 0.51204943, 0.5285176 , 0.5272734 , 0.51032214,\n",
       "         0.46837744, 0.49923632, 0.50001857, 0.53268378, 0.5313896 ,\n",
       "         0.51141622, 0.49935406, 0.4937465 , 0.48721894, 0.50644035,\n",
       "         0.48040634, 0.49128167, 0.49411286, 0.5449821 , 0.48549427,\n",
       "         0.48784552, 0.50856473, 0.47674292, 0.50257295, 0.48398484,\n",
       "         0.52633406, 0.50651124, 0.49839505, 0.50864625, 0.4991203 ,\n",
       "         0.52435365, 0.50594953, 0.50430357, 0.51778077, 0.50200356,\n",
       "         0.49805698, 0.50980022, 0.50746597, 0.46254894, 0.49397461,\n",
       "         0.55303806, 0.52653874, 0.51461901, 0.49573586, 0.53351777,\n",
       "         0.49557597, 0.5069912 , 0.49647251, 0.50693832, 0.50374305,\n",
       "         0.51276587, 0.53125602, 0.50936744, 0.49718543, 0.49570923,\n",
       "         0.53764188, 0.48866465, 0.49722887, 0.48991718, 0.47711542,\n",
       "         0.49836248, 0.50376371, 0.52318955, 0.51990124, 0.49592441,\n",
       "         0.49105163, 0.45418379, 0.53311447, 0.48008851, 0.52007354,\n",
       "         0.51246523, 0.51159201, 0.5040335 , 0.46723872, 0.49347921,\n",
       "         0.46234164, 0.49313507, 0.49190302, 0.47599994, 0.51098348,\n",
       "         0.53848401, 0.50783399, 0.52034127, 0.53303194, 0.48546138,\n",
       "         0.48620956, 0.47826934, 0.50031309, 0.47549185, 0.49099053,\n",
       "         0.51294088, 0.52001435, 0.48873173, 0.51983621, 0.48496482,\n",
       "         0.49607335, 0.48916742, 0.52685321, 0.4870212 , 0.4927935 ,\n",
       "         0.52248199, 0.50152765, 0.5374021 , 0.50849901, 0.49337884,\n",
       "         0.52379352, 0.46812402, 0.50373805, 0.4948212 , 0.4822338 ,\n",
       "         0.51336571, 0.50870824, 0.5336765 , 0.50473241, 0.53170858,\n",
       "         0.52464832, 0.49707869, 0.4872093 , 0.49387856, 0.49478019,\n",
       "         0.48348687, 0.49747915, 0.51156548, 0.49978218, 0.48725491,\n",
       "         0.48928227, 0.49122036, 0.54186171, 0.52276874]]),\n",
       " array([[1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "         1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "         1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "         0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "         1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "         0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "         0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "         1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "         1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "         0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "         1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "         0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "         0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "         1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "         1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "         1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "         0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "         1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "         0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "         1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "         0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "         0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "         1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "         0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "         0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "         1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "         0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "         1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "         0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "         1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "         0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_1_prob, v_1_bin = rbm.get_v_given_h(h_0_prob)\n",
    "v_1_prob, v_1_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.51912145, 0.57101545, 0.54124681, 0.57508801, 0.50042054,\n",
       "         0.51782055, 0.44547597, 0.48703537, 0.50017765, 0.50447032,\n",
       "         0.45391978, 0.70482323, 0.47039377, 0.52674292, 0.49390315,\n",
       "         0.50355785, 0.56572676, 0.53671538, 0.52246848, 0.52199877,\n",
       "         0.38453354, 0.54490468, 0.43824068, 0.60948713, 0.48945792,\n",
       "         0.47869123, 0.48865211, 0.4035239 , 0.48247302, 0.53410165,\n",
       "         0.49831314, 0.54721711, 0.44951284, 0.52692613, 0.48367667,\n",
       "         0.50117526, 0.54659916, 0.4741652 , 0.48071837, 0.50417887,\n",
       "         0.52729122, 0.50623876, 0.57916525, 0.50026158, 0.54005486,\n",
       "         0.52098616, 0.52447848, 0.50191599, 0.39588933, 0.50001755,\n",
       "         0.44123025, 0.49860757, 0.4502808 , 0.51812957, 0.44967458,\n",
       "         0.57939755, 0.52781212, 0.50832405, 0.46068523, 0.47033749,\n",
       "         0.55328493, 0.51155104, 0.51267123, 0.47428436, 0.49260477,\n",
       "         0.42084247, 0.52808219, 0.42136681, 0.60549858, 0.52271525,\n",
       "         0.53787636, 0.53416481, 0.49234312, 0.50176094, 0.56400396,\n",
       "         0.5382681 , 0.54456098, 0.54594268, 0.50112205, 0.47475907,\n",
       "         0.48366584, 0.44223339, 0.53144042, 0.51849218, 0.51705971,\n",
       "         0.54558077, 0.43324856, 0.43750256, 0.45855993, 0.53903303,\n",
       "         0.54421406, 0.46995093, 0.47199864, 0.50975071, 0.53893058,\n",
       "         0.48538025, 0.45866592, 0.47563571, 0.48859112, 0.53458061,\n",
       "         0.46391302, 0.4548356 , 0.54952664, 0.41061381, 0.48752527,\n",
       "         0.46481237, 0.51406161, 0.45847489, 0.47499544, 0.49829025,\n",
       "         0.48863986, 0.50418596, 0.47185261, 0.5714855 , 0.51229652,\n",
       "         0.47557795, 0.484243  , 0.45482892, 0.48856514, 0.45354832,\n",
       "         0.55950156, 0.48396393, 0.50158136, 0.56164932, 0.52174432,\n",
       "         0.52568149, 0.4620798 , 0.51347802, 0.48355895, 0.42763685,\n",
       "         0.49940489, 0.40666115, 0.51175838, 0.51349902, 0.47383849,\n",
       "         0.49568091, 0.53212457, 0.58967209, 0.4650597 , 0.47176211,\n",
       "         0.54597982, 0.46738052, 0.47453964, 0.41445017, 0.58306238,\n",
       "         0.47206861, 0.44697983, 0.46460395, 0.53050127, 0.49772402,\n",
       "         0.4259068 , 0.52963498, 0.51765675, 0.55287733, 0.56198372,\n",
       "         0.5223419 , 0.55361401, 0.42181939, 0.49052146, 0.49133158,\n",
       "         0.48889484, 0.52532657, 0.49229968, 0.43832665, 0.57339559,\n",
       "         0.40976428, 0.43488392, 0.45620767, 0.55954195, 0.48878864,\n",
       "         0.5518507 , 0.49535485, 0.49884127, 0.46670131, 0.49196073,\n",
       "         0.50236376, 0.54686727, 0.45496183, 0.44272427, 0.59214477,\n",
       "         0.49543919, 0.51202677, 0.5010242 , 0.45393409, 0.43113601,\n",
       "         0.47888566, 0.47889041, 0.5828745 , 0.59404406, 0.45117371,\n",
       "         0.54649618, 0.51893298, 0.48314014, 0.50766681, 0.48480602,\n",
       "         0.51307382, 0.58574653, 0.49588088, 0.45703311, 0.55398839]]),\n",
       " array([[1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "         1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "         0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "         0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "         0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "         0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "         1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "         1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "         1, 0]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_1_prob, h_1_bin = rbm.get_h_given_v(v_1_bin)\n",
    "h_1_prob, h_1_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm.update_params(v_0, h_0_bin, v_1_prob, h_1_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01140949, -0.00959612, -0.00564764, ...,  0.00719526,\n",
       "        -0.01769582, -0.00851558],\n",
       "       [-0.01005587, -0.00566965,  0.00064863, ..., -0.01776827,\n",
       "        -0.00714199,  0.00479012],\n",
       "       [ 0.00047399, -0.00457341,  0.01053612, ..., -0.0122025 ,\n",
       "         0.00602502, -0.00434662],\n",
       "       ...,\n",
       "       [ 0.00832434, -0.00977634,  0.01161037, ..., -0.01052388,\n",
       "        -0.00697904,  0.00113687],\n",
       "       [ 0.00059595, -0.01485199,  0.00287014, ...,  0.00276023,\n",
       "        -0.01165741,  0.00236436],\n",
       "       [-0.00519468,  0.0039604 , -0.00808148, ..., -0.0030103 ,\n",
       "        -0.00749945,  0.01783543]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm.weight_vh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_1_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2411461112.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[14], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    if (it % rbm.rf[\"period\"] == 0 or it ==  )and rbm.is_bottom:\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "if (it % rbm.rf[\"period\"] == 0 or it ==  )and rbm.is_bottom:\n",
    "    viz_rf(weights=rbm.weight_vh[:,rbm.rf[\"ids\"]].reshape((rbm.image_size[0],rbm.image_size[1],-1)), it=it, grid=rbm.rf[\"grid\"])\n",
    "\n",
    "# print progress\n",
    "\n",
    "if it % rbm.print_period == 0:\n",
    "    print (\"iteration=%7d recon_loss=%4.4f\"%(it, np.linalg.norm(v_1_bin - visible_trainset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AnndaKTH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

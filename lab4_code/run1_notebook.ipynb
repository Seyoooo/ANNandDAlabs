{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "\n",
    "class RestrictedBoltzmannMachine():\n",
    "    '''\n",
    "    For more details : A Practical Guide to Training Restricted Boltzmann Machines https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf\n",
    "    '''\n",
    "    def __init__(self, ndim_visible, ndim_hidden, is_bottom=False, image_size=[28,28], is_top=False, n_labels=10, batch_size=10):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          ndim_visible: Number of units in visible layer.\n",
    "          ndim_hidden: Number of units in hidden layer.\n",
    "          is_bottom: True only if this rbm is at the bottom of the stack in a deep belief net. Used to interpret visible layer as image data with dimensions \"image_size\".\n",
    "          image_size: Image dimension for visible layer.\n",
    "          is_top: True only if this rbm is at the top of stack in deep beleif net. Used to interpret visible layer as concatenated with \"n_label\" unit of label data at the end. \n",
    "          n_label: Number of label categories.\n",
    "          batch_size: Size of mini-batch.\n",
    "        \"\"\"\n",
    "       \n",
    "        self.ndim_visible = ndim_visible\n",
    "\n",
    "        self.ndim_hidden = ndim_hidden\n",
    "\n",
    "        self.is_bottom = is_bottom\n",
    "        if is_bottom : self.image_size = image_size\n",
    "        \n",
    "        self.is_top = is_top\n",
    "\n",
    "        if is_top : self.n_labels = 10\n",
    "\n",
    "        self.batch_size = batch_size        \n",
    "                \n",
    "        self.delta_bias_v = 0\n",
    "\n",
    "        self.delta_weight_vh = 0\n",
    "\n",
    "        self.delta_bias_h = 0\n",
    "\n",
    "        self.bias_v = np.random.normal(loc=0.0, scale=0.01, size=(self.ndim_visible))\n",
    "\n",
    "        self.weight_vh = np.random.normal(loc=0.0, scale=0.01, size=(self.ndim_visible,self.ndim_hidden))\n",
    "\n",
    "        self.bias_h = np.random.normal(loc=0.0, scale=0.01, size=(self.ndim_hidden))\n",
    "        \n",
    "        self.delta_weight_v_to_h = 0\n",
    "\n",
    "        self.delta_weight_h_to_v = 0        \n",
    "        \n",
    "        self.weight_v_to_h = None\n",
    "        \n",
    "        self.weight_h_to_v = None\n",
    "\n",
    "        self.learning_rate = 0.01\n",
    "        \n",
    "        self.momentum = 0.7\n",
    "\n",
    "        self.print_period = 5000\n",
    "        \n",
    "        self.rf = { # receptive-fields. Only applicable when visible layer is input data\n",
    "            \"period\" : 5000, # iteration period to visualize\n",
    "            \"grid\" : [5,5], # size of the grid\n",
    "            \"ids\" : np.random.randint(0,self.ndim_hidden,25) # pick some random hidden units\n",
    "            }\n",
    "        \n",
    "        return\n",
    "\n",
    "        \n",
    "    def cd1(self, visible_trainset, n_iterations=10000):\n",
    "        \n",
    "        \"\"\"Contrastive Divergence with k=1 full alternating Gibbs sampling\n",
    "\n",
    "        Args:\n",
    "          visible_trainset: training data for this rbm, shape is (size of training set, size of visible layer)\n",
    "          n_iterations: number of iterations of learning (each iteration learns a mini-batch)\n",
    "        \"\"\"\n",
    "\n",
    "        print (\"learning CD1\")\n",
    "        \n",
    "        n_samples = visible_trainset.shape[0]\n",
    "        index = 0 \n",
    "\n",
    "        for it in range(1):\n",
    "            # Select next mini-batch\n",
    "            next_index = index + self.batch_size\n",
    "\n",
    "            if next_index < n_samples:\n",
    "                v_0 = visible_trainset[index:next_index]\n",
    "            else:\n",
    "                v_0 = np.concatenate((visible_trainset[index:],visible_trainset[:next_index-n_samples]))\n",
    "                print(index)\n",
    "            index = next_index % n_samples\n",
    "\n",
    "\t        # [Done TASK 4.1] run k=1 alternating Gibbs sampling : v_0 -> h_0 ->  v_1 -> h_1.\n",
    "            # you may need to use the inference functions 'get_h_given_v' and 'get_v_given_h'.\n",
    "            # note that inference methods returns both probabilities and activations (samples from probablities) and you may have to decide when to use what.\n",
    "\n",
    "            print(v_0)\n",
    "\n",
    "            h_0_prob, h_0_bin = self.get_h_given_v(v_0)\n",
    "            v_1_prob, v_1_bin = self.get_v_given_h(h_0_prob)\n",
    "            h_1_prob, h_1_bin = self.get_h_given_v(v_1_bin)\n",
    "            \n",
    "            # [Done TASK 4.1] update the parameters using function 'update_params'\n",
    "            self.update_params(v_0, h_0_bin, v_1_prob, h_1_prob)\n",
    "\n",
    "            # visualize once in a while when visible layer is input images\n",
    "            \n",
    "            if it % self.rf[\"period\"] == 0 and self.is_bottom:\n",
    "                viz_rf(weights=self.weight_vh[:,self.rf[\"ids\"]].reshape((self.image_size[0],self.image_size[1],-1)), it=it, grid=self.rf[\"grid\"])\n",
    "\n",
    "            # print progress\n",
    "            \n",
    "            if it % self.print_period == 0 :\n",
    "                print (\"iteration=%7d recon_loss=%4.4f\"%(it, np.linalg.norm(visible_trainset - visible_trainset)))\n",
    "        \n",
    "        return\n",
    "    \n",
    "\n",
    "    def update_params(self,v_0,h_0,v_k,h_k):\n",
    "\n",
    "        \"\"\"Update the weight and bias parameters.\n",
    "\n",
    "        You could also add weight decay and momentum for weight updates.\n",
    "\n",
    "        Args:\n",
    "           v_0: activities or probabilities of visible layer (data to the rbm)\n",
    "           h_0: activities or probabilities of hidden layer\n",
    "           v_k: activities or probabilities of visible layer\n",
    "           h_k: activities or probabilities of hidden layer\n",
    "           all args have shape (size of mini-batch, size of respective layer)\n",
    "        \"\"\"\n",
    "\n",
    "        # [DONE TASK 4.1] get the gradients from the arguments (replace the 0s below) and update the weight and bias parameters\n",
    "        \n",
    "        self.delta_bias_v = self.learning_rate * np.mean(v_0 - v_k, axis=0)\n",
    "        self.delta_weight_vh = self.learning_rate * (v_0.T @ h_0 - v_k.T @ h_k) / v_0.shape[0]\n",
    "        self.delta_bias_h = self.learning_rate * np.mean(h_0 - h_k, axis=0)\n",
    "\n",
    "        assert self.delta_bias_v.shape[0] == v_0.shape[1]\n",
    "\n",
    "        self.bias_v += self.delta_bias_v\n",
    "        self.weight_vh += self.delta_weight_vh\n",
    "        self.bias_h += self.delta_bias_h\n",
    "        \n",
    "        return\n",
    "\n",
    "    def get_h_given_v(self, visible_minibatch):\n",
    "        \n",
    "        \"\"\"Compute probabilities p(h|v) and activations h ~ p(h|v) \n",
    "\n",
    "        Uses undirected weight \"weight_vh\" and bias \"bias_h\"\n",
    "        \n",
    "        Args: \n",
    "           visible_minibatch: shape is (size of mini-batch, size of visible layer)\n",
    "        Returns:        \n",
    "           tuple ( p(h|v) , h) \n",
    "           both are shaped (size of mini-batch, size of hidden layer)\n",
    "        \"\"\"\n",
    "        \n",
    "        assert self.weight_vh is not None\n",
    "\n",
    "        n_samples = visible_minibatch.shape[0]\n",
    "\n",
    "        # [Done TASK 4.1] compute probabilities and activations (samples from probabilities) of hidden layer (replace the zeros below) \n",
    "\n",
    "        probs = sigmoid(visible_minibatch @ self.weight_vh + self.bias_h)\n",
    "        binary_states = np.random.binomial(1, probs, size=None)\n",
    "        \n",
    "        return probs, binary_states\n",
    "\n",
    "\n",
    "    def get_v_given_h(self,hidden_minibatch):\n",
    "        \n",
    "        \"\"\"Compute probabilities p(v|h) and activations v ~ p(v|h)\n",
    "\n",
    "        Uses undirected weight \"weight_vh\" and bias \"bias_v\"\n",
    "        \n",
    "        Args: \n",
    "           hidden_minibatch: shape is (size of mini-batch, size of hidden layer)\n",
    "        Returns:        \n",
    "           tuple ( p(v|h) , v) \n",
    "           both are shaped (size of mini-batch, size of visible layer)\n",
    "        \"\"\"\n",
    "        \n",
    "        assert self.weight_vh is not None\n",
    "\n",
    "        n_samples = hidden_minibatch.shape[0]\n",
    "\n",
    "        if self.is_top:\n",
    "\n",
    "            \"\"\"\n",
    "            Here visible layer has both data and labels. Compute total input for each unit (identical for both cases), \\ \n",
    "            and split into two parts, something like support[:, :-self.n_labels] and support[:, -self.n_labels:]. \\\n",
    "            Then, for both parts, use the appropriate activation function to get probabilities and a sampling method \\\n",
    "            to get activities. The probabilities as well as activities can then be concatenated back into a normal visible layer.\n",
    "            \"\"\"\n",
    "\n",
    "            # [TODO TASK 4.1] compute probabilities and activations (samples from probabilities) of visible layer (replace the pass below). \\\n",
    "            # Note that this section can also be postponed until TASK 4.2, since in this task, stand-alone RBMs do not contain labels in visible layer.\n",
    "            \n",
    "            \n",
    "            pass\n",
    "            \n",
    "        else:\n",
    "                        \n",
    "            # [DONE TASK 4.1] compute probabilities and activations (samples from probabilities) of visible layer (replace the pass and zeros below)\n",
    "\n",
    "            probs = sigmoid(hidden_minibatch @ self.weight_vh.T + self.bias_v)\n",
    "            binary_states = np.random.binomial(1, probs, size=None)             \n",
    "\n",
    "            pass\n",
    "        \n",
    "        return probs, binary_states\n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\" rbm as a belief layer : the functions below do not have to be changed until running a deep belief net \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    def untwine_weights(self):\n",
    "        \n",
    "        self.weight_v_to_h = np.copy( self.weight_vh )\n",
    "        self.weight_h_to_v = np.copy( np.transpose(self.weight_vh) )\n",
    "        self.weight_vh = None\n",
    "\n",
    "    def get_h_given_v_dir(self,visible_minibatch):\n",
    "\n",
    "        \"\"\"Compute probabilities p(h|v) and activations h ~ p(h|v)\n",
    "\n",
    "        Uses directed weight \"weight_v_to_h\" and bias \"bias_h\"\n",
    "        \n",
    "        Args: \n",
    "           visible_minibatch: shape is (size of mini-batch, size of visible layer)\n",
    "        Returns:        \n",
    "           tuple ( p(h|v) , h) \n",
    "           both are shaped (size of mini-batch, size of hidden layer)\n",
    "        \"\"\"\n",
    "        \n",
    "        assert self.weight_v_to_h is not None\n",
    "\n",
    "        n_samples = visible_minibatch.shape[0]\n",
    "\n",
    "        # [TODO TASK 4.2] perform same computation as the function 'get_h_given_v' but with directed connections (replace the zeros below) \n",
    "        \n",
    "        return np.zeros((n_samples,self.ndim_hidden)), np.zeros((n_samples,self.ndim_hidden))\n",
    "\n",
    "\n",
    "    def get_v_given_h_dir(self,hidden_minibatch):\n",
    "\n",
    "\n",
    "        \"\"\"Compute probabilities p(v|h) and activations v ~ p(v|h)\n",
    "\n",
    "        Uses directed weight \"weight_h_to_v\" and bias \"bias_v\"\n",
    "        \n",
    "        Args: \n",
    "           hidden_minibatch: shape is (size of mini-batch, size of hidden layer)\n",
    "        Returns:        \n",
    "           tuple ( p(v|h) , v) \n",
    "           both are shaped (size of mini-batch, size of visible layer)\n",
    "        \"\"\"\n",
    "        \n",
    "        assert self.weight_h_to_v is not None\n",
    "        \n",
    "        n_samples = hidden_minibatch.shape[0]\n",
    "        \n",
    "        if self.is_top:\n",
    "\n",
    "            \"\"\"\n",
    "            Here visible layer has both data and labels. Compute total input for each unit (identical for both cases), \\ \n",
    "            and split into two parts, something like support[:, :-self.n_labels] and support[:, -self.n_labels:]. \\\n",
    "            Then, for both parts, use the appropriate activation function to get probabilities and a sampling method \\\n",
    "            to get activities. The probabilities as well as activities can then be concatenated back into a normal visible layer.\n",
    "            \"\"\"\n",
    "            \n",
    "            # [TODO TASK 4.2] Note that even though this function performs same computation as 'get_v_given_h' but with directed connections,\n",
    "            # this case should never be executed : when the RBM is a part of a DBN and is at the top, it will have not have directed connections.\n",
    "            # Appropriate code here is to raise an error (replace pass below)\n",
    "            \n",
    "            pass\n",
    "            \n",
    "        else:\n",
    "                        \n",
    "            # [TODO TASK 4.2] performs same computaton as the function 'get_v_given_h' but with directed connections (replace the pass and zeros below)             \n",
    "\n",
    "            pass\n",
    "            \n",
    "        return np.zeros((n_samples,self.ndim_visible)), np.zeros((n_samples,self.ndim_visible))        \n",
    "        \n",
    "    def update_generate_params(self,inps,trgs,preds):\n",
    "        \n",
    "        \"\"\"Update generative weight \"weight_h_to_v\" and bias \"bias_v\"\n",
    "        \n",
    "        Args:\n",
    "           inps: activities or probabilities of input unit\n",
    "           trgs: activities or probabilities of output unit (target)\n",
    "           preds: activities or probabilities of output unit (prediction)\n",
    "           all args have shape (size of mini-batch, size of respective layer)\n",
    "        \"\"\"\n",
    "\n",
    "        # [TODO TASK 4.3] find the gradients from the arguments (replace the 0s below) and update the weight and bias parameters.\n",
    "        \n",
    "        self.delta_weight_h_to_v += 0\n",
    "        self.delta_bias_v += 0\n",
    "        \n",
    "        self.weight_h_to_v += self.delta_weight_h_to_v\n",
    "        self.bias_v += self.delta_bias_v \n",
    "        \n",
    "        return\n",
    "    \n",
    "    def update_recognize_params(self,inps,trgs,preds):\n",
    "        \n",
    "        \"\"\"Update recognition weight \"weight_v_to_h\" and bias \"bias_h\"\n",
    "        \n",
    "        Args:\n",
    "           inps: activities or probabilities of input unit\n",
    "           trgs: activities or probabilities of output unit (target)\n",
    "           preds: activities or probabilities of output unit (prediction)\n",
    "           all args have shape (size of mini-batch, size of respective layer)\n",
    "        \"\"\"\n",
    "\n",
    "        # [TODO TASK 4.3] find the gradients from the arguments (replace the 0s below) and update the weight and bias parameters.\n",
    "\n",
    "        self.delta_weight_v_to_h += 0\n",
    "        self.delta_bias_h += 0\n",
    "\n",
    "        self.weight_v_to_h += self.delta_weight_v_to_h\n",
    "        self.bias_h += self.delta_bias_h\n",
    "        \n",
    "        return    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting a Restricted Boltzmann Machine..\n"
     ]
    }
   ],
   "source": [
    "image_size = [28,28]\n",
    "train_imgs,train_lbls,test_imgs,test_lbls = read_mnist(dim=image_size, n_train=60000, n_test=10000)\n",
    "\n",
    "''' restricted boltzmann machine '''\n",
    "\n",
    "print (\"\\nStarting a Restricted Boltzmann Machine..\")\n",
    "\n",
    "rbm = RestrictedBoltzmannMachine(ndim_visible=image_size[0]*image_size[1],\n",
    "                                    ndim_hidden=200,\n",
    "                                    is_bottom=True,\n",
    "                                    image_size=image_size,\n",
    "                                    is_top=False,\n",
    "                                    n_labels=10,\n",
    "                                    batch_size=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "visible_trainset = train_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = visible_trainset.shape[0]\n",
    "index = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_index = index + rbm.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if next_index < n_samples:\n",
    "    v_0 = visible_trainset[index:next_index]\n",
    "else:\n",
    "    v_0 = np.concatenate((visible_trainset[index:],visible_trainset[:next_index-n_samples]))\n",
    "    print(index)\n",
    "index = next_index % n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "         0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "         0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "         1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "         0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "         0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "         0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "         0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 1]]),\n",
       " array([[0.42375982, 0.64900002, 0.41266226, 0.66588423, 0.62112613,\n",
       "         0.63833397, 0.41005009, 0.66247842, 0.41006205, 0.65846291,\n",
       "         0.64859548, 0.43232736, 0.63624507, 0.64916533, 0.44694191,\n",
       "         0.66556608, 0.42728521, 0.63659416, 0.66363595, 0.62636174,\n",
       "         0.63191945, 0.6657586 , 0.41294186, 0.65933082, 0.64764307,\n",
       "         0.64565571, 0.43711027, 0.45098669, 0.64007686, 0.64271218,\n",
       "         0.39701573, 0.44702773, 0.42178368, 0.66325126, 0.67166813,\n",
       "         0.66374933, 0.67018301, 0.64554684, 0.64954795, 0.41977924,\n",
       "         0.66538565, 0.42787094, 0.66706584, 0.63959303, 0.43694195,\n",
       "         0.43694561, 0.44287327, 0.42607171, 0.65880311, 0.67894616,\n",
       "         0.65181545, 0.43257264, 0.65184929, 0.66928971, 0.43684757,\n",
       "         0.42678659, 0.65896304, 0.65201863, 0.64609258, 0.43041147,\n",
       "         0.59626006, 0.41123517, 0.40887084, 0.70420423, 0.42823222,\n",
       "         0.63888991, 0.6573094 , 0.45050854, 0.6285238 , 0.65191674,\n",
       "         0.65549298, 0.657243  , 0.63134636, 0.6774699 , 0.42560733,\n",
       "         0.68691399, 0.42518697, 0.67884806, 0.41947593, 0.63726667,\n",
       "         0.65739882, 0.42902034, 0.39463096, 0.59917212, 0.64720492,\n",
       "         0.4114606 , 0.64063773, 0.65924818, 0.45447916, 0.42878201,\n",
       "         0.68231958, 0.62053821, 0.68046089, 0.43694157, 0.66738251,\n",
       "         0.65603863, 0.47333715, 0.39197353, 0.43294844, 0.65463237,\n",
       "         0.67581232, 0.63298075, 0.65932837, 0.67790371, 0.63082325,\n",
       "         0.62636393, 0.44488823, 0.42625953, 0.64382025, 0.66916943,\n",
       "         0.62501887, 0.63044912, 0.38733738, 0.45191452, 0.47194495,\n",
       "         0.4478121 , 0.41084817, 0.65332484, 0.6689124 , 0.45620024,\n",
       "         0.43487117, 0.45152458, 0.65743743, 0.4282873 , 0.42687402,\n",
       "         0.4365989 , 0.43874767, 0.61133614, 0.68386888, 0.67831647,\n",
       "         0.6754689 , 0.69451926, 0.67345898, 0.43428021, 0.42946946,\n",
       "         0.44871496, 0.64827884, 0.7009354 , 0.66855243, 0.6630759 ,\n",
       "         0.40414526, 0.41417516, 0.40898942, 0.42157318, 0.67003007,\n",
       "         0.4605067 , 0.4443698 , 0.68567602, 0.4183468 , 0.6408375 ,\n",
       "         0.409122  , 0.65880869, 0.41529718, 0.45610951, 0.44535099,\n",
       "         0.64931657, 0.44062697, 0.68169012, 0.65271228, 0.43507293,\n",
       "         0.6574588 , 0.43692047, 0.4234745 , 0.4127071 , 0.42794175,\n",
       "         0.40121259, 0.6383514 , 0.37227313, 0.43687176, 0.42692297,\n",
       "         0.63908886, 0.66726413, 0.4710567 , 0.65748566, 0.63908464,\n",
       "         0.68471141, 0.41270212, 0.66054391, 0.63967999, 0.41815206,\n",
       "         0.63077872, 0.43556656, 0.41961821, 0.38493909, 0.65184985,\n",
       "         0.68797362, 0.43460533, 0.67553797, 0.66547877, 0.63930729,\n",
       "         0.42770197, 0.6585142 , 0.65860339, 0.63134221, 0.64908481,\n",
       "         0.69408165, 0.69500854, 0.67394705, 0.46348336, 0.67641102]]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_0_prob, h_0_bin = rbm.get_h_given_v(v_0)\n",
    "h_0_bin, h_0_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.42140182, 0.42899015, 0.42564638, 0.44398823, 0.44669331,\n",
       "         0.40661191, 0.45251741, 0.45547469, 0.44351111, 0.45525818,\n",
       "         0.42286012, 0.42253531, 0.4575813 , 0.42862119, 0.40701121,\n",
       "         0.45170332, 0.44968311, 0.41937595, 0.42773317, 0.44844417,\n",
       "         0.44930286, 0.44844064, 0.44112257, 0.42627967, 0.4213384 ,\n",
       "         0.41782192, 0.43200656, 0.43453885, 0.42779708, 0.43479559,\n",
       "         0.44051703, 0.44357403, 0.45877859, 0.42386065, 0.4289647 ,\n",
       "         0.41255279, 0.39793455, 0.42995658, 0.40446651, 0.47949412,\n",
       "         0.4340866 , 0.46025246, 0.4633722 , 0.4042163 , 0.41924759,\n",
       "         0.39892437, 0.44083864, 0.40945827, 0.4323377 , 0.4058061 ,\n",
       "         0.43413873, 0.41822923, 0.44871141, 0.43476685, 0.43772876,\n",
       "         0.40874741, 0.44006944, 0.42519373, 0.44022164, 0.42368145,\n",
       "         0.42920335, 0.41827244, 0.41576927, 0.39025489, 0.40699716,\n",
       "         0.43001328, 0.41136934, 0.45383459, 0.42346628, 0.43360883,\n",
       "         0.43644805, 0.44732804, 0.44048922, 0.43639399, 0.43978892,\n",
       "         0.43818845, 0.42859494, 0.46994658, 0.41522393, 0.42829552,\n",
       "         0.45834667, 0.4342911 , 0.4352012 , 0.42012272, 0.44970975,\n",
       "         0.40981399, 0.45088064, 0.42052709, 0.4248556 , 0.41120924,\n",
       "         0.42223418, 0.42687617, 0.43059051, 0.4245373 , 0.43880459,\n",
       "         0.45197925, 0.4390019 , 0.44153219, 0.47954766, 0.41347847,\n",
       "         0.4226903 , 0.4468667 , 0.45604317, 0.41327357, 0.42575637,\n",
       "         0.40253699, 0.44424958, 0.48922237, 0.44904932, 0.43618204,\n",
       "         0.41100035, 0.41967296, 0.44199529, 0.4610472 , 0.4356905 ,\n",
       "         0.43481215, 0.45196383, 0.39222861, 0.43141411, 0.46175547,\n",
       "         0.43490667, 0.42915953, 0.41123815, 0.40708037, 0.42073201,\n",
       "         0.43502168, 0.42883924, 0.41032975, 0.40758529, 0.42494502,\n",
       "         0.44248258, 0.39510917, 0.40253596, 0.42623556, 0.42289146,\n",
       "         0.41405775, 0.42456548, 0.4372175 , 0.43261291, 0.43458812,\n",
       "         0.42584862, 0.44129509, 0.42731079, 0.43889734, 0.41681384,\n",
       "         0.4221382 , 0.43917759, 0.41997168, 0.42160094, 0.4392392 ,\n",
       "         0.45288128, 0.43464039, 0.40626798, 0.44929056, 0.41669842,\n",
       "         0.45672686, 0.51585082, 0.53882515, 0.58500892, 0.45574728,\n",
       "         0.58097558, 0.62774249, 0.56608169, 0.5248436 , 0.44349269,\n",
       "         0.41829751, 0.43502746, 0.42064733, 0.40656566, 0.42405341,\n",
       "         0.45246987, 0.42887773, 0.44828927, 0.42003082, 0.41134722,\n",
       "         0.43364595, 0.45310855, 0.43783097, 0.49799651, 0.53694047,\n",
       "         0.56756785, 0.61295922, 0.65155097, 0.57264691, 0.61839091,\n",
       "         0.62191878, 0.5437307 , 0.56681266, 0.62596335, 0.61209934,\n",
       "         0.54888897, 0.44811139, 0.41925514, 0.417619  , 0.43495045,\n",
       "         0.46539571, 0.42022168, 0.42275633, 0.41678131, 0.42279193,\n",
       "         0.42880605, 0.4416494 , 0.43346652, 0.48206987, 0.61030135,\n",
       "         0.6324209 , 0.61957772, 0.63689855, 0.5968025 , 0.61066332,\n",
       "         0.63151064, 0.64730711, 0.63550284, 0.59774775, 0.49096999,\n",
       "         0.4954401 , 0.50440385, 0.43703633, 0.48300572, 0.43071388,\n",
       "         0.43188365, 0.41722022, 0.41908673, 0.43964969, 0.42476176,\n",
       "         0.45097458, 0.4342245 , 0.41603479, 0.43480784, 0.46194854,\n",
       "         0.44190584, 0.42269173, 0.59086912, 0.61468671, 0.62948336,\n",
       "         0.60569845, 0.6155108 , 0.60341301, 0.60107119, 0.5339977 ,\n",
       "         0.63000949, 0.63278945, 0.40298208, 0.40892219, 0.43727922,\n",
       "         0.42028864, 0.40859877, 0.42404847, 0.40713837, 0.46774985,\n",
       "         0.41397174, 0.43555144, 0.41357185, 0.43507327, 0.40087728,\n",
       "         0.43984636, 0.43525961, 0.43899669, 0.47310032, 0.42399183,\n",
       "         0.51245229, 0.55421416, 0.47986895, 0.62036128, 0.60150586,\n",
       "         0.59740794, 0.43212859, 0.4285678 , 0.4508555 , 0.51289429,\n",
       "         0.46597326, 0.45139219, 0.44007887, 0.43929906, 0.41347005,\n",
       "         0.41296628, 0.44572912, 0.4350729 , 0.4549236 , 0.42745433,\n",
       "         0.44202414, 0.43520186, 0.44097042, 0.42734163, 0.42994282,\n",
       "         0.4398705 , 0.43208641, 0.47551239, 0.44757608, 0.43183875,\n",
       "         0.42246988, 0.5587316 , 0.59137999, 0.51155128, 0.42972505,\n",
       "         0.43179301, 0.42831772, 0.44655302, 0.43801543, 0.43441226,\n",
       "         0.44479018, 0.46087025, 0.40878264, 0.41644667, 0.46236551,\n",
       "         0.44389943, 0.42443742, 0.42679938, 0.43179699, 0.43877398,\n",
       "         0.46723898, 0.41852152, 0.41822122, 0.46165981, 0.41067992,\n",
       "         0.4536742 , 0.43444834, 0.43272962, 0.38241611, 0.54049697,\n",
       "         0.632823  , 0.58299191, 0.44068832, 0.4299228 , 0.44127804,\n",
       "         0.41308268, 0.4152451 , 0.42617759, 0.45202881, 0.42348551,\n",
       "         0.42977556, 0.40068662, 0.4773921 , 0.45776593, 0.43726955,\n",
       "         0.41370177, 0.44174144, 0.41487995, 0.43225904, 0.39879356,\n",
       "         0.46109818, 0.40805595, 0.43558744, 0.42807913, 0.43839953,\n",
       "         0.42940023, 0.41640004, 0.42876804, 0.57694086, 0.59825089,\n",
       "         0.49017301, 0.43647066, 0.45289686, 0.39697984, 0.43690551,\n",
       "         0.44836829, 0.40084128, 0.4449063 , 0.40554314, 0.39473272,\n",
       "         0.45777244, 0.43898238, 0.42813003, 0.41695234, 0.43715792,\n",
       "         0.41698857, 0.41161105, 0.43245063, 0.43250931, 0.40983025,\n",
       "         0.43989308, 0.38457918, 0.43804665, 0.43395077, 0.43229638,\n",
       "         0.41580071, 0.45212308, 0.6093684 , 0.58261355, 0.54343608,\n",
       "         0.49980348, 0.44128842, 0.45190214, 0.44113046, 0.42443541,\n",
       "         0.42125615, 0.43944591, 0.39312383, 0.41600515, 0.40881158,\n",
       "         0.42904104, 0.4004307 , 0.41644472, 0.42461758, 0.42819248,\n",
       "         0.41197001, 0.45891129, 0.39909023, 0.42514015, 0.44325315,\n",
       "         0.42179005, 0.44934476, 0.40470807, 0.43454013, 0.42630681,\n",
       "         0.47100519, 0.59094111, 0.59108401, 0.61553059, 0.52147412,\n",
       "         0.45027379, 0.40906825, 0.42205233, 0.44179335, 0.40537759,\n",
       "         0.44483924, 0.45136829, 0.41265434, 0.42121689, 0.37134561,\n",
       "         0.43945034, 0.4379681 , 0.43024798, 0.42053113, 0.43018832,\n",
       "         0.44689119, 0.45579349, 0.44739196, 0.38433311, 0.40103708,\n",
       "         0.40644092, 0.42585411, 0.43266048, 0.4170881 , 0.46737752,\n",
       "         0.57329542, 0.62766524, 0.62210229, 0.5436908 , 0.43871813,\n",
       "         0.47746312, 0.44335346, 0.44064149, 0.41637796, 0.43037281,\n",
       "         0.43232634, 0.39322935, 0.40625763, 0.41377402, 0.41923434,\n",
       "         0.42633952, 0.42782844, 0.43371193, 0.39422233, 0.43055382,\n",
       "         0.43671625, 0.45996174, 0.4230363 , 0.43094802, 0.40761334,\n",
       "         0.45357602, 0.44327246, 0.43198522, 0.42901232, 0.4614915 ,\n",
       "         0.61623043, 0.6094475 , 0.57029662, 0.4305394 , 0.4582665 ,\n",
       "         0.43621664, 0.4306468 , 0.46455944, 0.42444293, 0.42289783,\n",
       "         0.46652722, 0.43373704, 0.4130383 , 0.4151691 , 0.4029868 ,\n",
       "         0.42974883, 0.43190408, 0.44881028, 0.45609685, 0.42147833,\n",
       "         0.42330972, 0.42969799, 0.45695699, 0.43624025, 0.39863387,\n",
       "         0.43305109, 0.45279153, 0.43658129, 0.57979925, 0.60160846,\n",
       "         0.62753877, 0.45774474, 0.4280624 , 0.44216838, 0.42515177,\n",
       "         0.47284915, 0.40501293, 0.41205242, 0.44352729, 0.40780595,\n",
       "         0.44850595, 0.45214727, 0.46083925, 0.39786388, 0.4288791 ,\n",
       "         0.44324801, 0.420622  , 0.45154177, 0.41863553, 0.44657023,\n",
       "         0.38517225, 0.42270359, 0.41377619, 0.46776518, 0.50519732,\n",
       "         0.56467211, 0.60574401, 0.59784242, 0.56899263, 0.44923942,\n",
       "         0.44358203, 0.38697925, 0.43655355, 0.43421102, 0.41128408,\n",
       "         0.40676212, 0.42036729, 0.40453566, 0.42226899, 0.42505545,\n",
       "         0.43803225, 0.42525628, 0.44280023, 0.4238717 , 0.41844805,\n",
       "         0.44483678, 0.43051522, 0.40177494, 0.45233205, 0.42379865,\n",
       "         0.53949733, 0.56487297, 0.60016412, 0.61169198, 0.60818478,\n",
       "         0.61469973, 0.55339081, 0.41952708, 0.44157354, 0.43425296,\n",
       "         0.44272548, 0.43454365, 0.44585382, 0.44540397, 0.41728985,\n",
       "         0.43572417, 0.42520012, 0.42445103, 0.43475135, 0.44046894,\n",
       "         0.41505605, 0.42334117, 0.4060705 , 0.40681531, 0.43621942,\n",
       "         0.47892389, 0.5034945 , 0.56930248, 0.60912667, 0.60639141,\n",
       "         0.63036292, 0.62690077, 0.56689319, 0.502987  , 0.43923246,\n",
       "         0.37775203, 0.43902004, 0.42925062, 0.44361802, 0.42783879,\n",
       "         0.47413552, 0.43182384, 0.41468544, 0.42070188, 0.40701195,\n",
       "         0.42784712, 0.40547781, 0.40807191, 0.43249491, 0.46988665,\n",
       "         0.46619846, 0.45068185, 0.46400293, 0.59552737, 0.61694365,\n",
       "         0.61399354, 0.63186591, 0.62430533, 0.58777394, 0.46333428,\n",
       "         0.4448629 , 0.4059341 , 0.4382985 , 0.4264587 , 0.4405847 ,\n",
       "         0.45811981, 0.39962713, 0.39794061, 0.39394001, 0.431563  ,\n",
       "         0.45747679, 0.41166128, 0.44626307, 0.44810368, 0.43921335,\n",
       "         0.42727993, 0.41153145, 0.47851565, 0.5756072 , 0.59947285,\n",
       "         0.60771213, 0.61563786, 0.6151375 , 0.62813315, 0.58986146,\n",
       "         0.47344674, 0.44077491, 0.41368066, 0.42379799, 0.41377931,\n",
       "         0.44316674, 0.42389158, 0.42339538, 0.45325036, 0.42990564,\n",
       "         0.42473394, 0.42394675, 0.42331075, 0.43258768, 0.41435329,\n",
       "         0.3966297 , 0.44199994, 0.46059336, 0.47769061, 0.57283705,\n",
       "         0.58890735, 0.59866838, 0.60274395, 0.61572301, 0.63982452,\n",
       "         0.61078259, 0.50797137, 0.43311341, 0.42158496, 0.4311943 ,\n",
       "         0.40636114, 0.44420622, 0.40946292, 0.44222058, 0.45823105,\n",
       "         0.41548543, 0.46321839, 0.40768387, 0.45929224, 0.41379499,\n",
       "         0.41241371, 0.41274094, 0.43927591, 0.38025163, 0.44918033,\n",
       "         0.42192593, 0.52021797, 0.61235627, 0.59517163, 0.62277244,\n",
       "         0.60304424, 0.53015786, 0.49722925, 0.44809648, 0.44042223,\n",
       "         0.41849879, 0.45369729, 0.4525144 , 0.41610246, 0.44394627,\n",
       "         0.42475716, 0.41328851, 0.46084526, 0.42406567, 0.42417042,\n",
       "         0.45235855, 0.44192213, 0.41189682, 0.44313568, 0.3853578 ,\n",
       "         0.41382106, 0.43475928, 0.43238007, 0.40313539, 0.43432527,\n",
       "         0.43325757, 0.41130075, 0.42108734, 0.43815925, 0.43065937,\n",
       "         0.45336698, 0.42364242, 0.43725314, 0.40773737, 0.44728723,\n",
       "         0.44932289, 0.43741302, 0.39426413, 0.44570777, 0.43064095,\n",
       "         0.45554302, 0.4342982 , 0.42554434, 0.44639527, 0.4548874 ,\n",
       "         0.40884296, 0.43547705, 0.43771688, 0.42132399, 0.45251663,\n",
       "         0.44778084, 0.41650337, 0.47167536, 0.41863717, 0.45222059,\n",
       "         0.443726  , 0.4087304 , 0.42962692, 0.39991537, 0.43777864,\n",
       "         0.41316593, 0.42345022, 0.41677395, 0.406825  , 0.42399995,\n",
       "         0.46213886, 0.42294757, 0.41371968, 0.43707108, 0.42476584,\n",
       "         0.43918435, 0.42872284, 0.4120406 , 0.41471879, 0.41869568,\n",
       "         0.41867165, 0.43331839, 0.4521655 , 0.46260371, 0.42327897,\n",
       "         0.44185304, 0.4165708 , 0.43873863, 0.42585583, 0.43939782,\n",
       "         0.41716671, 0.44708965, 0.42226912, 0.42840787, 0.43170799,\n",
       "         0.44331657, 0.4610685 , 0.46006773, 0.42730226, 0.43267675,\n",
       "         0.40271263, 0.43869364, 0.43939227, 0.44054352, 0.45059193,\n",
       "         0.45074937, 0.42321964, 0.4079342 , 0.44748113]]),\n",
       " array([[1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "         0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "         0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "         0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "         1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "         1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "         0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "         0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "         1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "         1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "         0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "         1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "         1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "         1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "         0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "         1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "         0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "         1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "         1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "         1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "         0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "         0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "         0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "         1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "         0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_1_prob, v_1_bin = rbm.get_v_given_h(h_0_prob)\n",
    "v_1_prob, v_1_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.30155607, 0.40950653, 0.23923101, 0.4114678 , 0.40579124,\n",
       "         0.45078854, 0.24904557, 0.37202085, 0.28105247, 0.39926623,\n",
       "         0.43167611, 0.31739041, 0.39277636, 0.35833381, 0.31885275,\n",
       "         0.46923783, 0.26825435, 0.45423854, 0.43395257, 0.36481621,\n",
       "         0.46663084, 0.41336159, 0.31898821, 0.46061837, 0.38671542,\n",
       "         0.40512551, 0.27564614, 0.35877992, 0.47737553, 0.37663021,\n",
       "         0.26272877, 0.27354466, 0.24458893, 0.48127048, 0.39993492,\n",
       "         0.47245357, 0.47187091, 0.46513932, 0.38928976, 0.22950789,\n",
       "         0.41718493, 0.29052067, 0.37546832, 0.37580324, 0.35032997,\n",
       "         0.32710236, 0.31965284, 0.28378791, 0.39511843, 0.53232844,\n",
       "         0.44166113, 0.27120126, 0.41518702, 0.43400946, 0.24596732,\n",
       "         0.28753851, 0.48116252, 0.43202324, 0.44804412, 0.24833096,\n",
       "         0.37837707, 0.22444136, 0.30844939, 0.51341208, 0.2602912 ,\n",
       "         0.3759609 , 0.36731032, 0.2348389 , 0.45930542, 0.43492936,\n",
       "         0.47081139, 0.41186881, 0.41281311, 0.38891975, 0.34949928,\n",
       "         0.39794633, 0.25248919, 0.3692986 , 0.26954585, 0.47027377,\n",
       "         0.49869244, 0.27828702, 0.26791094, 0.38486873, 0.44406177,\n",
       "         0.29422807, 0.45264905, 0.40008794, 0.29588837, 0.2635475 ,\n",
       "         0.36561383, 0.37939262, 0.4240785 , 0.27252963, 0.41055439,\n",
       "         0.43427774, 0.31832181, 0.28097894, 0.24321628, 0.41913586,\n",
       "         0.38095443, 0.43597592, 0.48261587, 0.46076782, 0.44119584,\n",
       "         0.39157216, 0.31891054, 0.23873891, 0.38668194, 0.36341637,\n",
       "         0.38737301, 0.37778051, 0.25040145, 0.26975449, 0.314013  ,\n",
       "         0.29607745, 0.25410166, 0.44800101, 0.4615669 , 0.31252906,\n",
       "         0.3029044 , 0.29058868, 0.39320516, 0.28303411, 0.24648543,\n",
       "         0.25054118, 0.32691234, 0.38654618, 0.36126785, 0.37592118,\n",
       "         0.48696861, 0.48206134, 0.45815006, 0.26226789, 0.26026711,\n",
       "         0.30575038, 0.38960299, 0.39953726, 0.48540352, 0.42945257,\n",
       "         0.27137601, 0.26762454, 0.26359804, 0.30798339, 0.4472915 ,\n",
       "         0.32155729, 0.30572108, 0.41826252, 0.28257719, 0.39096123,\n",
       "         0.28942191, 0.44284641, 0.25248509, 0.29034671, 0.32950436,\n",
       "         0.36664298, 0.23684678, 0.41963941, 0.40785839, 0.29098883,\n",
       "         0.42212023, 0.34306312, 0.29372197, 0.24383188, 0.29782399,\n",
       "         0.27952474, 0.36673474, 0.24882452, 0.25956632, 0.30789719,\n",
       "         0.40234814, 0.4127357 , 0.24522947, 0.43736385, 0.47945601,\n",
       "         0.42052819, 0.33195064, 0.49413838, 0.5070379 , 0.36997304,\n",
       "         0.40473543, 0.26986737, 0.29410514, 0.29390402, 0.513542  ,\n",
       "         0.42595199, 0.23488497, 0.40176994, 0.43756366, 0.43757961,\n",
       "         0.20999325, 0.41657445, 0.43116557, 0.46151143, 0.36546455,\n",
       "         0.41281633, 0.5165973 , 0.45722022, 0.25615036, 0.47094112]]),\n",
       " array([[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "         0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "         0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "         1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "         0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "         1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "         0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_1_prob, h_1_bin = rbm.get_h_given_v(v_1_bin)\n",
    "h_1_prob, h_1_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm.update_params(v_0, h_0_bin, v_1_prob, h_1_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00087062, -0.01935744, -0.01793766, ...,  0.00667793,\n",
       "        -0.01252888,  0.01143045],\n",
       "       [-0.01256875, -0.00406537, -0.00106786, ..., -0.00918752,\n",
       "        -0.01540138, -0.01647382],\n",
       "       [ 0.01666596, -0.00778327, -0.00301419, ..., -0.00499257,\n",
       "        -0.02001859, -0.01423958],\n",
       "       ...,\n",
       "       [ 0.01158802,  0.00778681,  0.0045401 , ..., -0.00092714,\n",
       "        -0.01814085,  0.00241645],\n",
       "       [ 0.00128996, -0.00919223, -0.01783769, ..., -0.00508061,\n",
       "        -0.01508978, -0.01934256],\n",
       "       [-0.00186642,  0.00010269,  0.00043879, ...,  0.00496276,\n",
       "        -0.00461471, -0.01877712]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm.weight_vh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_1_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=      0 recon_loss=4672.0340\n"
     ]
    }
   ],
   "source": [
    "if (it % rbm.rf[\"period\"] == 0 or it ==  )and rbm.is_bottom:\n",
    "    viz_rf(weights=rbm.weight_vh[:,rbm.rf[\"ids\"]].reshape((rbm.image_size[0],rbm.image_size[1],-1)), it=it, grid=rbm.rf[\"grid\"])\n",
    "\n",
    "# print progress\n",
    "\n",
    "if it % rbm.print_period == 0:\n",
    "    print (\"iteration=%7d recon_loss=%4.4f\"%(it, np.linalg.norm(v_1_bin - visible_trainset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AnndaKTH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
